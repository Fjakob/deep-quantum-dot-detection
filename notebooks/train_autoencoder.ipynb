{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlurRdVC02qd"
      },
      "source": [
        "Jupyter Notebook to train a residual autoencoder for peak signal reconstruction.\n",
        "\n",
        "Specifically designed for *Google Colab*.\n",
        "\n",
        "Author: **Fabian Jakob**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrzVpgAh5nxu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random as rnd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, flatten, sigmoid\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IXtX7gvDDdC"
      },
      "source": [
        "For GPU support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1w6mQTPBzzs",
        "outputId": "66209ae7-c7a3-458d-e698-a25c236a2928"
      },
      "outputs": [],
      "source": [
        "if not torch.cuda.is_available():\n",
        "  print(\"Set google colab runtime to GPU!\")\n",
        "else:\n",
        "  device = \"cuda\"\n",
        "  print(\"Using \" + device)\n",
        "  device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOmbNWSy66AK"
      },
      "source": [
        "\n",
        "Define Neural Network Encoder/Decoder Structures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKg7F72c6sY5"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\" Residual Block, consisting of two channel and size invariant convolutions plus shortcut. \"\"\"\n",
        "    def __init__(self, channel_num, kernel_size, mode='encode'):\n",
        "        super().__init__()\n",
        "\n",
        "        padding = int((kernel_size-1)/2)\n",
        "        if mode=='encode':\n",
        "            conv1 = nn.Conv1d(channel_num, channel_num, kernel_size, padding=padding)\n",
        "            conv2 = nn.Conv1d(channel_num, channel_num, kernel_size, padding=padding)\n",
        "        elif mode=='decode':\n",
        "            conv1 = nn.ConvTranspose1d(channel_num, channel_num, kernel_size, padding=padding)\n",
        "            conv2 = nn.ConvTranspose1d(channel_num, channel_num, kernel_size, padding=padding)\n",
        "        \n",
        "        self.conv_block1 = nn.Sequential(conv1, nn.BatchNorm1d(channel_num), nn.ReLU()) \n",
        "        self.conv_block2 = nn.Sequential(conv2, nn.BatchNorm1d(channel_num), nn.ReLU()) \n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    \"\"\" Residual Unit consisting of multiple residual Blocks. \"\"\"\n",
        "    def __init__(self, channel_in, channel_out, kernel_size, mode='encode'):\n",
        "        super().__init__()\n",
        "\n",
        "        # if necessary, perform dimension transformation to output channel\n",
        "        if channel_in == channel_out:\n",
        "            self.dimension_mismatch = False\n",
        "        else:\n",
        "            self.dimension_mismatch = True\n",
        "            if mode=='encode':\n",
        "                self.dim_transition = nn.Conv1d(channel_in, channel_out, kernel_size=1)\n",
        "            elif mode=='decode':\n",
        "                self.dim_transition = nn.ConvTranspose1d(channel_in, channel_out, kernel_size=1)\n",
        "\n",
        "        self.res_blk1 = ResidualBlock(channel_out, kernel_size, mode)\n",
        "        self.res_blk2 = ResidualBlock(channel_out, kernel_size, mode)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dim_transition(x) if self.dimension_mismatch else x\n",
        "        x = self.res_blk1(x)\n",
        "        x = self.res_blk2(x)\n",
        "        return(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\" Encoder neural network consisting of 4 convolutional layers. \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.pool   = nn.MaxPool1d(3)\n",
        "        self.conv1  = nn.Conv1d(1, 16, 7)\n",
        "        self.conv2  = nn.Conv1d(16, 32, 6)\n",
        "        self.conv3  = nn.Conv1d(32, 64, 6)\n",
        "        self.conv4  = nn.Conv1d(64, 128, 5)\n",
        "        self.dense1 = nn.Linear(1280, latent_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = self.pool(self.activation(self.conv1(input)))\n",
        "        out = self.bn1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.pool(self.activation(self.conv2(out)))\n",
        "        out = self.bn2(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.pool(self.activation(self.conv3(out)))\n",
        "        out = self.bn3(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.pool(self.activation(self.conv4(out)))\n",
        "        out = self.bn4(out)\n",
        "        out = self.drop(out)\n",
        "        out = flatten(out, 1)\n",
        "        out = self.dense1(out)\n",
        "        return out  \n",
        "\n",
        "\n",
        "class ResidualEncoder(nn.Module):\n",
        "    \"\"\" Encoder neural network consisting of 4 residual unis. \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.pool   = nn.MaxPool1d(3)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.resUnit1 = ResidualStack(1, 16, kernel_size=3)\n",
        "        self.resUnit2 = ResidualStack(16, 32, kernel_size=3)\n",
        "        self.resUnit3 = ResidualStack(32, 64, kernel_size=3)\n",
        "        self.resUnit4 = ResidualStack(64, 128, kernel_size=3)\n",
        "        self.dense    = nn.Linear(128*12, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.resUnit1(x))       \n",
        "        x = self.pool(self.resUnit2(x))\n",
        "        x = self.pool(self.resUnit3(x))\n",
        "        x = self.pool(self.resUnit4(x))\n",
        "        x = flatten(x, 1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Decoder neural network consisting of 4 inverted convolutional layers. \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dense1   = nn.Linear(latent_dim, 128*36)\n",
        "        self.conv1T   = nn.ConvTranspose1d(128, 64, 7, stride=3)\n",
        "        self.conv2T   = nn.ConvTranspose1d(64, 32, 6, stride=3)\n",
        "        self.conv3T   = nn.ConvTranspose1d(32, 16, 6, stride=3)\n",
        "        self.conv4T   = nn.ConvTranspose1d(16, 1, 5)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.bn4 = nn.BatchNorm1d(16)\n",
        "            \n",
        "    def forward(self, input):\n",
        "        out = self.activation(self.dense1(input))\n",
        "        out = out.view(-1, 128, 36)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(self.conv1T(out))\n",
        "        out = self.bn2(out)\n",
        "        out = self.activation(self.conv2T(out))\n",
        "        out = self.bn3(out)\n",
        "        out = self.activation(self.conv3T(out))\n",
        "        out = self.bn4(out)\n",
        "        out = self.conv4T(out)\n",
        "        return out  \n",
        "\n",
        "class DeepDecoder(nn.Module):\n",
        "    \"\"\" Decoder neural network consisting of 5 inverted convolutional layers. \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dense1   = nn.Linear(latent_dim, 128*36)\n",
        "        self.conv1T   = nn.ConvTranspose1d(128, 64, 7, stride=3)\n",
        "        self.conv2T   = nn.ConvTranspose1d(64, 32, 6, stride=3)\n",
        "        self.conv3T   = nn.ConvTranspose1d(32, 16, 6, stride=3)\n",
        "        self.conv4T   = nn.ConvTranspose1d(16, 8, 3, padding=1)\n",
        "        self.conv5T   = nn.ConvTranspose1d(8, 4, 3, padding=1)\n",
        "        self.conv6T   = nn.ConvTranspose1d(4, 1, 5)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.bn4 = nn.BatchNorm1d(16)\n",
        "        self.bn5 = nn.BatchNorm1d(8)\n",
        "        self.bn6 = nn.BatchNorm1d(4)\n",
        "        self.epsilon = 1e-12\n",
        "            \n",
        "    def forward(self, input):\n",
        "        out = self.activation(self.dense1(input))\n",
        "        out = out.view(-1, 128, 36)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(self.conv1T(out))\n",
        "        out = self.bn2(out)\n",
        "        out = self.activation(self.conv2T(out))\n",
        "        out = self.bn3(out)\n",
        "        out = self.activation(self.conv3T(out))\n",
        "        out = self.bn4(out)\n",
        "        out = self.activation(self.conv4T(out))\n",
        "        out = self.bn5(out)\n",
        "        out = self.activation(self.conv5T(out))\n",
        "        out = self.bn6(out)\n",
        "        out = (1+self.epsilon)*sigmoid(self.conv6T(out)) - self.epsilon/2\n",
        "        return out  \n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\" Autoencoder for dimensionality reduction. \"\"\"\n",
        "    def __init__(self, latent_dim=12):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = ResidualEncoder(latent_dim)\n",
        "        self.decoder = DeepDecoder(latent_dim)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        encoded = self.encoder(input)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzi1JtxKDNrL"
      },
      "source": [
        "Data loading and preparation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwpDPiS-6znq"
      },
      "outputs": [],
      "source": [
        "def loadDataSet(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        X = pickle.load(f)\n",
        "    return X\n",
        "\n",
        "def createDataset(X, device=\"cpu\", batch_size=128, test_split=0.2):\n",
        "    # split to test / train\n",
        "    n_data = len(X)\n",
        "    n_test = int(test_split*n_data)\n",
        "    n_train = n_data - n_test\n",
        "\n",
        "    X = torch.tensor(X, device=device)\n",
        "    dataSet = TensorDataset(X.float())\n",
        "\n",
        "    train_dataset, test_dataset = random_split(dataSet, [n_train, n_test])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wz0HpAWDQHr"
      },
      "source": [
        "Setup Printing functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNxuddlR62_R"
      },
      "outputs": [],
      "source": [
        "def printTrainingHistory(latent_dim, schedule_epochs=False):\n",
        "    file = \"train_history_{0}.pickle\".format(latent_dim)\n",
        "    if os.path.isfile(file):\n",
        "        with open(file, 'rb') as f:\n",
        "            hist, _ = pickle.load(f)\n",
        "    else: \n",
        "        print(\"No history!\")\n",
        "        return\n",
        "    train_loss = [epoch[0] for epoch in hist]\n",
        "    test_loss = [epoch[1] for epoch in hist]\n",
        "    print(\"Training loss: {}, Test loss: {}\".format(train_loss[-1], test_loss[-1]))\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.plot(train_loss, label='Train loss')\n",
        "    ax.grid()\n",
        "    ax.plot(test_loss, label='Test loss')\n",
        "    ax.legend()\n",
        "    ax.set_ylim((0,35/10000))\n",
        "    plt.show()\n",
        "\n",
        "def printSamplePredict(test_loader, autoencoder, device, sample=1):\n",
        "    n_subplots = 1\n",
        "    plt.figure(figsize=(16,4))\n",
        "    with torch.no_grad():\n",
        "        jdx=0\n",
        "        for batch in test_loader:\n",
        "            jdx+=1\n",
        "            batch_in = torch.reshape(batch[0].to(device), (batch[0].size()[0], 1, 1024))\n",
        "            batch_enc = autoencoder.encoder(batch_in)\n",
        "            batch_out = autoencoder(batch_in)\n",
        "            if jdx==sample:\n",
        "                for idx in range(n_subplots):\n",
        "                    r = rnd.randint(0, len(batch))\n",
        "                    plt.subplot(n_subplots,2,idx+1)\n",
        "                    plt.plot(np.squeeze(batch_in[r].cpu().detach().numpy()), label='Original')\n",
        "                    plt.plot(np.squeeze(batch_out[r].cpu().detach().numpy()), '--', label='Reconstructed')\n",
        "                    plt.legend()\n",
        "                    plt.subplot(n_subplots,2,idx+2)\n",
        "                    plt.plot(np.squeeze(batch_enc[r].cpu().detach().numpy()))\n",
        "                break;\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LL85P6v7L5q"
      },
      "source": [
        "Train the Autoencoder:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IJJTEwTprx6",
        "outputId": "09404ef6-2168-4e83-9ed5-1070216a3c06"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "print(\"----------- Loading data -----------\")\n",
        "X = loadDataSet('data_w30_unlabeled_normalized_augmented.pickle')\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YVNDhr2c7Cgp",
        "outputId": "c95414b5-6d75-47d6-869a-952adf4229fa"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "epochs = 200\n",
        "batch_size = 512\n",
        "learning_rate = 0.01\n",
        "latent_dim = 24\n",
        "\n",
        "# dataset loader\n",
        "train_loader, test_loader = createDataset(X, device, batch_size, test_split=0.2)\n",
        "\n",
        "# instanciate model\n",
        "print(\"---------- Creating model ----------\")\n",
        "autoencoder = Autoencoder(latent_dim).to(device)\n",
        "summary(autoencoder, (1,1024))\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# learning rate schedule variables\n",
        "count = 0\n",
        "loss_storage = []\n",
        "schedule_epochs = []\n",
        "schedule_horizon = 30\n",
        "train_stagnation = 0.5/10000\n",
        "\n",
        "print(\"---------- Start training ----------\")\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # learning rate schedule:\n",
        "    count += 1\n",
        "    if count > schedule_horizon:\n",
        "\n",
        "        # track variation in training loss\n",
        "        recent_loss = [train_loss for train_loss, test_loss in loss_storage[-schedule_horizon:]]\n",
        "        loss_change = np.std(recent_loss)\n",
        "\n",
        "        if loss_change < train_stagnation:\n",
        "            learning_rate = learning_rate / 2\n",
        "            optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "            print(f\"-------- Learning rate schedule performed from {2*learning_rate} to {learning_rate} ----------\")\n",
        "            schedule_horizon += 5\n",
        "            schedule_epochs.append(epoch)\n",
        "            count = 0\n",
        "            train_stagnation = train_stagnation / 2\n",
        "\n",
        "    loss = 0\n",
        "    for batch_original in train_loader:\n",
        "        # preprocess batch as model input\n",
        "        batch_original = torch.reshape(batch_original[0].to(device), (batch_original[0].size()[0], 1, 1024)).to(device)\n",
        "        \n",
        "        # reset gradient\n",
        "        optimizer.zero_grad() \n",
        "        \n",
        "        # compute loss\n",
        "        batch_decoded = autoencoder(batch_original)\n",
        "        train_loss = loss_function(batch_original, batch_decoded)\n",
        "        \n",
        "        # compute gradient + perform gradient decent\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss += train_loss.item()\n",
        "\n",
        "    # epoch training loss\n",
        "    loss = loss / len(train_loader)\n",
        "    \n",
        "    # compute test loss\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        for test_batch in test_loader:\n",
        "            test_batch = torch.reshape(test_batch[0].to(device), (test_batch[0].size()[0], 1, 1024))\n",
        "\n",
        "            test_batch_decoded = autoencoder(test_batch)\n",
        "            test_loss += loss_function(test_batch, test_batch_decoded).item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "    \n",
        "    loss_storage.append((loss, test_loss))\n",
        "        \n",
        "    print(\"epoch : {}/{}, loss = {:.2f}, test loss = {:.2f}\".format(epoch + 1, epochs, loss*10000, test_loss*10000)) \n",
        "\n",
        "\n",
        "print(\"--------- Printing results ----------\")\n",
        "\n",
        "for idx in range(8):\n",
        "    printSamplePredict(test_loader, autoencoder, device, sample=idx+1)\n",
        "\n",
        "\n",
        "print(\"---------- Saving results ----------\")\n",
        "\n",
        "filename = f\"autoencoder{latent_dim}.pth\"\n",
        "\n",
        "torch.save(autoencoder.state_dict(), filename)\n",
        "files.download(filename)\n",
        "\n",
        "with open(f\"train_history_{latent_dim}.pickle\", \"wb\") as fp:\n",
        "    pickle.dump((loss_storage, schedule_epochs), fp)\n",
        "files.download(f\"train_history_{latent_dim}.pickle\")\n",
        "\n",
        "printTrainingHistory(latent_dim)\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "ngVcwDjFzmXK",
        "outputId": "0fecbbf5-04c0-4ccb-8b0b-9d55e881244c"
      },
      "outputs": [],
      "source": [
        "printTrainingHistory(latent_dim)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "770d094a551ac2ee51119be4a7b5fbc513b7711c9e3b59c0b2e0e70b799619a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
